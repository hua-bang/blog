import{_ as a,c as s,o as t,a1 as p}from"./chunks/framework.BUQJ8Vm8.js";const d=JSON.parse('{"title":"本地部署 DeepSeek R1","description":"","frontmatter":{"title":"本地部署 DeepSeek R1","customTag":"blog>互联网","date":"2025.01.29","editLink":true},"headers":[],"relativePath":"blog/deploy-deepseek-r1.md","filePath":"blog/deploy-deepseek-r1.md","lastUpdated":1740920856000}'),i={name:"blog/deploy-deepseek-r1.md"};function r(n,e,o,l,h,c){return t(),s("div",null,e[0]||(e[0]=[p(`<h1 id="本地部署-deepseek-r1" tabindex="-1">本地部署 DeepSeek R1 <a class="header-anchor" href="#本地部署-deepseek-r1" aria-label="Permalink to &quot;本地部署 DeepSeek R1&quot;">​</a></h1><p>年前一两周， <code>DeepSeek</code> 刚上线深度搜索的时候，就进行体验了。</p><p>当时感觉下来，整体感觉挺惊艳的，其中，让我感觉惊艳的是思考过程。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202022.png" alt="image.png"></p><p>以至于当时我上周主要都是在用 <code>DeepSeek</code> ，以及和推荐朋友进行使用。</p><p>但到年前前几天的时候，发现周围的人都在讨论 <code>DeepSeek</code>, 甚至在中外 App Store 霸榜了，后续也上了新闻，连 OpenAI 的 Sam 也出来发声了。</p><p>随即之后就是，一大堆人涌入使用 <code>DeepSeek</code>，同时可能也存在一些外部的网络攻击，导致其服务不稳定，远远没有上周那么丝滑。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202038.png" alt="image.png"></p><p>于是，如果 <code>DeepSeek</code> 是个私有化部署的服务的话，那么就不会有上述问题了。</p><p>以及 <code>DeepSeek</code> 的 <code>R1</code> 模型是开源的，我们可以通过 <code>Ollama</code> 来进行模型的私有化部署。</p><p>下方我们就来自己试一试。</p><h2 id="模型部署" tabindex="-1">模型部署 <a class="header-anchor" href="#模型部署" aria-label="Permalink to &quot;模型部署&quot;">​</a></h2><p>这里的模型部署分为三步：安装 ollama + 部署 DeepSeek R1 模型 + API 调用</p><h3 id="安装-ollama" tabindex="-1">安装 ollama <a class="header-anchor" href="#安装-ollama" aria-label="Permalink to &quot;安装 ollama&quot;">​</a></h3><p>进入 <a href="https://ollama.com/" target="_blank" rel="noreferrer">https://ollama.com/</a> 网站，点击下载按钮，目前支持 macOS, Linux 和 Windows 操作系统。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202052.png" alt="image.png"></p><p>下载安装后，双击 Ollama 图标，会开启一个后台服务。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202105.png" alt="image.png"></p><h3 id="部署-deepseek-r1-模型" tabindex="-1">部署 DeepSeek R1 模型 <a class="header-anchor" href="#部署-deepseek-r1-模型" aria-label="Permalink to &quot;部署 DeepSeek R1 模型&quot;">​</a></h3><p>在 Ollama 上 搜索 DeepSeek， 很容易找到 DeepSeek-R1 的模型。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202122.png" alt="image.png"></p><p>我们进入 <a href="https://ollama.com/library/deepseek-r1%EF%BC%8C%E5%85%B7%E4%BD%93%E7%9C%8B%E7%9C%8B" target="_blank" rel="noreferrer">https://ollama.com/library/deepseek-r1，具体看看</a> DeepSeek-R1 的配置，目前有多个参数可以选择， 1.5b - 671b，可以根据电脑的配置去进行选择。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202153.png" alt="image.png"></p><p>这里，我们跑个最小参数的模型, deepseek-r1:1.5b。</p><p>打开终端，执行命令 <code>ollama run deepseek-r1:1.5b</code></p><p>会自己去拉取 deepseek-r1:1.5b 模型，这里需要等待一下。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202207.png" alt="image.png"></p><p>拉取完之后，会默认支持你进行输入。</p><p>我们先进行简单地输入，可以看到，下方并没有“思考过程”，直接返回结果了。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202222.png" alt="image.png"></p><p>我们问一个难一点的问题，可以看到“思考过程”</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202236.png" alt="image.png"></p><p>这样子，我们就能在终端进行模型的调用了，但这种用法目前太局限了。</p><h3 id="api-调用" tabindex="-1">API 调用 <a class="header-anchor" href="#api-调用" aria-label="Permalink to &quot;API 调用&quot;">​</a></h3><p>ollama 也提供了 api 的调用方式，具体可以看 <a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank" rel="noreferrer">ollama api</a> 的文档。</p><p>实际上， ollama 在执行后，会在本地开一个进程，用于 暴露 API。我们执行下方命令，可以看到支持输出，并且支持流式传输。</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">http://localhost:11434/api/generat</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">e</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;model&quot;: &quot;deepseek-r1:1.5b&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}&#39;</span></span></code></pre></div><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202257.png" alt="image.png"></p><h2 id="可视化界面" tabindex="-1">可视化界面 <a class="header-anchor" href="#可视化界面" aria-label="Permalink to &quot;可视化界面&quot;">​</a></h2><p>上方我们已经跑通了模型的服务，但这样子使用还是很麻烦，如果有一个 Web 交互界面就更好了。刚好 <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noreferrer">open-webui</a> 提供了 Web 交互界面的能力， 并且支持 docker 部署。</p><p>所以这里我们主要分为两步 Docker 部署 + open-webui 安装</p><h3 id="docker-安装" tabindex="-1">Docker 安装 <a class="header-anchor" href="#docker-安装" aria-label="Permalink to &quot;Docker 安装&quot;">​</a></h3><p>这里可以进入 <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noreferrer">https://docs.docker.com/engine/install/</a> 进入安装。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202316.png" alt="image.png"></p><p>安装完成后，可以使用 docker -v 检测 docker 是否安装成功。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202327.png" alt="image.png"></p><h3 id="安装-open-webui" tabindex="-1">安装 <strong>Open WebUI</strong> <a class="header-anchor" href="#安装-open-webui" aria-label="Permalink to &quot;安装 **Open WebUI**&quot;">​</a></h3><p>进入 <a href="https://github.com/open-webui/open-webui?tab=readme-ov-file#quick-start-with-docker-" target="_blank" rel="noreferrer">open-webui</a> 页面，可以看到有通过 docker 部署的说明文档。</p><p>支持执行下方命令</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --add-host=host.docker.internal:host-gateway</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:main</span></span></code></pre></div><p>执行完后，可以看到 <code>Docker Desktop</code> 多了个容器。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202350.png" alt="image.png"></p><p>点击上述的端口链接，就能看到可视化界面了。</p><p><img src="https://raw.githubusercontent.com/hua-bang/assert-store/master/20250129202407.png" alt="image.png"></p><p>这样子，我们就能在 Web 界面调用 DeepSeek R1 了， 上方的界面也和 DeepSeek 有点相似，并且这个是本地部署的，意味着你可以离线使用，并且免费。</p><h2 id="写在最后" tabindex="-1">写在最后 <a class="header-anchor" href="#写在最后" aria-label="Permalink to &quot;写在最后&quot;">​</a></h2><p>这几天体验下来，<code>DeepSeek</code> 的深度思考的功能体验不错，通过 <code>ollama</code> 去进行部署的体验也挺有趣，这也是我近期日常使用 DeepSeek 的原因。</p><p>我也觉得 DeepSeek 确实很惊艳，但目前有些媒体也许把它写得过于夸张。所以与其去从别人那里听说这些使用效果，不如自己动手进行体验。</p>`,58)]))}const g=a(i,[["render",r]]);export{d as __pageData,g as default};
